{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d683bd",
   "metadata": {},
   "source": [
    "# PAC-Bayesian Theory Meets Bayesian Inference\n",
    "\n",
    "authors:\n",
    "\n",
    "Ramón Daniel REGUEIRO ESPIÑO \n",
    "( ramondaniel1999@gmail.com )\n",
    "\n",
    "María Magdalena SÁNCHEZ DEL RÍO AUFRERE DE LA PREGNE\n",
    "( srapmaria@gmail.com )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b46481",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cba487",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bayesian_regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesian_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianRegression\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bayesian_regression'"
     ]
    }
   ],
   "source": [
    "from bayesian_regression import BayesianRegression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, log, exp\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb0ba93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m (kl \u001b[38;5;241m-\u001b[39m log(delta)) \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mc))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m45\u001b[39m)\n\u001b[1;32m      4\u001b[0m     nb_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m      5\u001b[0m     nb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    np.random.seed(45)\n",
    "\n",
    "    nb_test = 100000\n",
    "    nb_dim = 20\n",
    "    noise = 1/3\n",
    "    sigma_prior = .1\n",
    "    sigma_post = sqrt(2)\n",
    "    w_norm = .5\n",
    "\n",
    "    bound_param_a = 1.\n",
    "    bound_param_b = 4.\n",
    "\n",
    "    bound_param_c = (sigma_prior ** 2) / (sigma_post ** 2)\n",
    "    bound_param_s = sqrt(nb_dim * sigma_prior**2 + w_norm**2 + (1 - bound_param_c) * noise**2) / sigma_post\n",
    "\n",
    "    print(\"Bound parameters: a=%f, b=%f, s=%f, c=%f\" % (bound_param_a, bound_param_b, bound_param_s, bound_param_c))\n",
    "\n",
    "    assert bound_param_c < 1\n",
    "\n",
    "    def create_data(n, _w=None):\n",
    "        return create_linear_data(n, nb_dim, w=_w, sigma_noise=noise, w_norm=w_norm)\n",
    "\n",
    "    print('Creating test data...')\n",
    "    x_test, y_test, w_data = create_data(nb_test)\n",
    "\n",
    "    nb_train_list = np.logspace(1, 6, 20, dtype=int)\n",
    "    nb_params_train = len(nb_train_list)\n",
    "\n",
    "    catoni_list, alquier_sqrt_list, alquier_n_list, subgamma_list,  train_loss_list, test_loss_list, kl_list = [np.zeros(nb_params_train) for _ in range(7)]\n",
    "\n",
    "    print('Creating training data...')\n",
    "    x_all, y_all, _ = create_data(nb_train_list[-1], w_data)\n",
    "\n",
    "    for i, nb_train in enumerate(nb_train_list):\n",
    "        x, y = x_all[:nb_train, :], y_all[:nb_train]\n",
    "        print('Training on %d datapoints...' % nb_train)\n",
    "        clf = BayesianRegression(sigma_prior, sigma_post)\n",
    "        clf.fit(x, y)\n",
    "\n",
    "        train_loss_list[i] = clf.calc_gibbs_nll_loss(x, y)\n",
    "        test_loss_list[i] = clf.calc_gibbs_nll_loss(x_test, y_test)\n",
    "\n",
    "        kl_list[i] = clf.calc_kullback_leibler()\n",
    "        alquier_sqrt_list[i] = bound_alquier_sqrt(train_loss_list[i], kl_list[i], nb_train, bound_param_a, bound_param_b)\n",
    "        alquier_n_list[i] = bound_alquier_n(train_loss_list[i], kl_list[i], nb_train, bound_param_a, bound_param_b)\n",
    "        catoni_list[i] = bound_catoni(train_loss_list[i], kl_list[i], nb_train, bound_param_a, bound_param_b)\n",
    "        subgamma_list[i] = bound_subgamma(train_loss_list[i], kl_list[i], nb_train, bound_param_s, bound_param_c)\n",
    "\n",
    "    print('done!')\n",
    "\n",
    "    plt.semilogx(nb_train_list, alquier_n_list, 'gray', linestyle='-', linewidth=4, marker='o', markersize=10, label=r\"Alquier et al's $[a,b]$ bound (Theorem 3 + Eq 13)\")\n",
    "    plt.semilogx(nb_train_list, alquier_sqrt_list, '-', linewidth=4, marker='o', markersize=10, label=r\"Alquier et al's $[a,b]$ bound (Theorem 3 + Eq 14)\")\n",
    "    plt.semilogx(nb_train_list, catoni_list, '-', linewidth=4, marker='o', markersize=10, label=r\"Catoni's $[a,b]$ bound (Corollary 2)\")\n",
    "    plt.semilogx(nb_train_list, subgamma_list, '-', linewidth=4, marker='o', markersize=10, label=r\"sub-gamma bound (Corollary 5)\")\n",
    "\n",
    "    plt.semilogx(nb_train_list, test_loss_list, 'k--', linewidth=4, marker='s', markersize=10, label=r'$\\mathbf{E}_{\\theta\\sim\\hat\\rho^*} \\mathcal{L}_{\\mathcal D}^{\\,\\ell_{\\rm nll}}(\\theta)$ (test loss)')\n",
    "    plt.semilogx(nb_train_list, train_loss_list, 'g-', linewidth=4, marker='s', markersize=10, label=r'$\\mathbf{E}_{\\theta\\sim\\hat\\rho^*} \\widehat\\mathcal{L}_{X,Y}^{\\,\\ell_{\\rm nll}}(\\theta)$ (train loss)')\n",
    "\n",
    "    plt.xlabel('$n$')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_linear_data(n=100, d=10, sigma_x=1., w=None, w_norm=1., sigma_noise=0.1):\n",
    "    if w is None:\n",
    "        w = np.random.rand(d)\n",
    "        w /= np.linalg.norm(w) / w_norm\n",
    "\n",
    "    x = np.random.multivariate_normal(np.zeros(d), sigma_x ** 2 * np.eye(d), n) # Sample gaussian data\n",
    "    noise = np.random.normal(scale=sigma_noise, size=n)\n",
    "    y = np.dot(x, w) + noise\n",
    "    return x, y, w\n",
    "\n",
    "\n",
    "def create_linear_data_into_ball(n=100, d=10, w=None, w_norm=1, sigma_noise=.1, radius=1):\n",
    "    if w is None:\n",
    "        w = np.random.rand(d)\n",
    "        w /= (np.linalg.norm(w) / w_norm)\n",
    "\n",
    "    nb_outside = n\n",
    "    norms = 2 * radius * np.ones(nb_outside)\n",
    "    x = np.ones((n, d))\n",
    "\n",
    "    while nb_outside > 0:\n",
    "        x[norms > radius, :] = np.random.randn(nb_outside, d)  # Sample normal data\n",
    "        norms = np.linalg.norm(x, axis=1)\n",
    "        nb_outside = np.sum(norms > radius) # reject data outside the ball\n",
    "\n",
    "    noise = np.random.normal(scale=sigma_noise, size=n)\n",
    "    y = np.dot(x, w) + noise\n",
    "    return x, y, w\n",
    "\n",
    "\n",
    "def bound_alquier_sqrt(loss, kl, n, a, b, delta=0.05):\n",
    "    return loss + (1./sqrt(n)) * (kl - log(delta) + 0.5 * (b - a) ** 2)\n",
    "\n",
    "\n",
    "def bound_alquier_n(loss, kl, n, a, b, delta=0.05):\n",
    "    return loss + (1./n) * (kl - log(delta)) + 0.5 * (b - a) ** 2\n",
    "\n",
    "\n",
    "def bound_catoni(loss, kl, n, a, b, delta=0.05):\n",
    "    x = loss - a + (kl - log(delta)) / n\n",
    "    z = (b - a) / (1-exp(a-b))\n",
    "    return a + z * (1 - exp(-x))\n",
    "\n",
    "\n",
    "def bound_subgamma(loss, kl, n, s, c, delta=0.05):\n",
    "    return loss + (1./n) * (kl - log(delta)) + s ** 2 / (2*(1-c))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da196c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
